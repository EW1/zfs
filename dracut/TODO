The plan for my fork of zfs-dracut is simple.  Dracut is moving to systemd to perform quite a few things.

So, we'll look at what we need to do to boot a system compatibly with systemd in Dracut.


If systemd is included in the initrd:

   In the hostonly case:

      Upon initrd generation, we:

      - check()
        - extend the lists of host_devs and host_fs
          - devs needs to contain all pool vdevs of the pools containing / and /usr
            and active swap devices
          - host_fs needs to contain the fake pool/fs device mapped to "zfs"

      - install() and friends
          - store pool cache file and hostid
          - determine which of the devices required by initrd.target are stored in ZFS
          - add the corresponding pools and vdevs to initrd.target, properly ordered
            - in addition to normal ordering, all units must go before dracut mount stage
          - determine which of the pools contains the root dataset as expressed by mtab
            - and also /usr, if separate usr is available
          - add the corresponding pool and vdevs to initrd.target, properly ordered
            - in addition to normal ordering, all units must go before dracut mount stage
          - add the module import as a prerequisite for the above, turning off autoimport

      Upon system boot:

      - in the parse stage: we determine the root dataset and pool (or auto)
      - in the mount stage: we wait until the pool with the root dataset appears
        (technically we should not need to wait since we ordered them before dracut mount)
        - then if zfs auto
          - find out bootfs
          - mount root dataset
          - mount the /usr dataset using the logic we use today for multiOS
        - else
          - mount the root dataset in question
          - mount the /usr dataset using the logic we use today for multiOS

   In the non-hostonly case:

      Upon initrd generation, we:

      - store the hostid
      - store the zpool.cache file for consultation later
      - add a generator (see below)

      Upon system boot:

      - in the generator stage:
        - if root ZFS:
          - use zdb to find out the devices of the root pool specified in the root=
            or all pools if ZFS=auto
          - add the corresponding pools and vdevs to initrd.target, properly ordered
            - in addition to normal ordering, all units must go before dracut mount stage
      - in the parse stage: we determine the root dataset and pool (or auto)
      - in the mount stage: we wait until the pool with the root dataset appears, or
        all pools have been imported in the case of ZFS auto
        (technically we should not need to wait since we ordered them before dracut mount)
        - then if zfs auto
          - find out bootfs
          - mount root dataset
          - mount the /usr dataset using the logic we use today for multiOS
        - else
          - mount the root dataset in question
          - mount the /usr dataset using the logic we use today for multiOS

Without systemd:

  - similar to the above, but instead of using generators and units, we use dracut facilities
    to wait for devices, and then import the involved pools once these devices are available


This needs to be QA'd against F17, F18 and F19, in all four combinations as outlined above.
Complex root pools and non-root pools need to be generated, above luks-encrypted files,
and that needs to be the test bed.


Plan is to do the systemd version first, QA that in hostonly and not, then the non-systemd version
(for which I need to see if it can be turned off and still works in F19) then QA that,
then QA and backport for F18 and F17.


Without a zpool.cache, there must still be something that can be done for some scenarios, like
attempting to research the availability of the pool using zdb against the stated pool in the
command line, repeatedly, until it yields the availability of devices from the MOS configuration
